# -*- coding: utf-8 -*-
"""assignment07.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ewmFcZElSfUciQgWLvB3appnU2tDQI9i

# Setup

Starter Directions:

1. Download the startup files from the assignment page.
    * Specifically this link: https://canvas.northwestern.edu/courses/101722/files/7323541/download?wrap=1
2. Create a folder in your drive called '422_data_files'
3. Open the files in folder 'cats_dogs_64-128'
4. Upload both 'cats_1000_64_64_1.npy' and 'dogs_1000_64_64_1.npy' to that folder.
5. Run this code and remember to authenticate yourself through your NU account.

## Import Modules
"""

# coding: utf-8

# Initial deep neural network set-up from 
# GeÃÅron, A. 2017. Hands-On Machine Learning with Scikit-Learn 
#    & TensorFlow: Concepts, Tools, and Techniques to Build 
#    Intelligent Systems. Sebastopol, Calif.: O'Reilly. 
#    [ISBN-13 978-1-491-96229-9] 
#    Source code available at https://github.com/ageron/handson-ml
#    See file 10_introduction_to_artificial_neural_networks.ipynb 
#    Revised from MNIST to Cats and Dogs to begin Assignment 7
#    #CatsDogs# comment lines show additions/revisions for Cats and Dogs

# To support both python 2 and python 3
from __future__ import division, print_function, unicode_literals

# Common imports for our work
import pandas as pd
import os 
import numpy as np
import tensorflow as tf
import time

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, AveragePooling2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""## Start Script Total Runtime"""

script_start = time.time()

"""## Settings"""

RANDOM_SEED = 9999

# To make output stable across runs
def reset_graph(seed= RANDOM_SEED):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

#CatsDogs# Old dimensions from MNIST no loger apply
#CatsDogs# height = 28
#CatsDogs# width = 28
height = 64
width = 64

#CatsDogs# 
# Documentation on npy binary format for saving numpy arrays for later use
#     https://towardsdatascience.com/
#             why-you-should-start-using-npy-file-more-often-df2a13cc0161
# Under the working directory, data files are in directory cats_dogs_64_128 
# Read in cats and dogs grayscale 64x64 files to create training data


cats_100_file_path = '/content/drive/My Drive/422_data_files/cats_1000_64_64_1.npy'
dogs_1000_file_path = '/content/drive/My Drive/422_data_files/dogs_1000_64_64_1.npy'

cats_1000_64_64_1 = np.load(cats_100_file_path)
dogs_1000_64_64_1 = np.load(dogs_1000_file_path)

"""# Startup Code Material"""

from matplotlib import pyplot as plt  # for display of images
def show_grayscale_image(image):
    plt.imshow(image, cmap='gray')
    plt.axis('off')
    plt.show()
# Examine first cat and first dog grayscale images
show_grayscale_image(cats_1000_64_64_1[0,:,:,0])
show_grayscale_image(dogs_1000_64_64_1[0,:,:,0])

# 300 and 100 nodes for layers 1 and 2 as used with MNIST from Geron
n_hidden1 = 300
n_hidden2 = 100

channels = 1  # When working with color images use channels = 3

n_inputs = height * width

#CatsDogs# Has two output values # MNIST had ten digits n_outputs = 10  
n_outputs = 2  # binary classification for Cats and Dogs, 1 output node 0/1

reset_graph()

# dnn... Deep neural network model from Geron Chapter 10
# Note that this model makes no use of the fact that we have
# pixel data arranged in rows and columns
# So a 64x64 matrix of raster values becomes a vector of 4096 input variables
X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
y = tf.placeholder(tf.int32, shape=(None), name="y")

def neuron_layer(X, n_neurons, name, activation=None):
    with tf.name_scope(name):
        n_inputs = int(X.get_shape()[1])
        stddev = 2 / np.sqrt(n_inputs)
        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)
        W = tf.Variable(init, name="kernel")
        b = tf.Variable(tf.zeros([n_neurons]), name="bias")
        Z = tf.matmul(X, W) + b
        if activation is not None:
            return activation(Z)
        else:
            return Z

with tf.name_scope("dnn"):
    hidden1 = neuron_layer(X, n_hidden1, name="hidden1",
                           activation=tf.nn.relu)
    hidden2 = neuron_layer(hidden1, n_hidden2, name="hidden2",
                           activation=tf.nn.relu)
    logits = neuron_layer(hidden2, n_outputs, name="outputs")

with tf.name_scope("loss"):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,
                                                              logits=logits)
    loss = tf.reduce_mean(xentropy, name="loss")

learning_rate = 0.01

with tf.name_scope("train"):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    training_op = optimizer.minimize(loss)

with tf.name_scope("eval"):
    correct = tf.nn.in_top_k(logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

init = tf.global_variables_initializer()
saver = tf.train.Saver()

# Work the data for cats and dogs numpy arrays 
# These numpy arrays were generated in previous data prep work
# Stack the numpy arrays for the inputs
X_cat_dog = np.concatenate((cats_1000_64_64_1, dogs_1000_64_64_1), axis = 0) 
X_cat_dog = X_cat_dog.reshape(-1,width*height) # note coversion to 4096 inputs

# Scikit Learn for min-max scaling of the data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(np.array([0., 255.]).reshape(-1,1))
X_cat_dog_min_max = scaler.transform(X_cat_dog)

# Define the labels to be used 1000 cats = 0 1000 dogs = 1
y_cat_dog = np.concatenate((np.zeros((1000), dtype = np.int32), 
                      np.ones((1000), dtype = np.int32)), axis = 0)

# Scikit Learn for random splitting of the data  
from sklearn.model_selection import train_test_split

# Random splitting of the data in to training (80%) and test (20%)  
X_train, X_test, y_train, y_test = \
    train_test_split(X_cat_dog_min_max, y_cat_dog, test_size=0.20, 
                     random_state = RANDOM_SEED)

init = tf.global_variables_initializer()    

n_epochs = 50
batch_size = 100

with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        for iteration in range(y_train.shape[0] // batch_size):
            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]
            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})
        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)

        save_path = saver.save(sess, "./my_catdog_model")

"""# Preprocessing

## Data reshaping

Reshape data to rank 4 to fit in Keras models.
"""

reshaped_x_train = X_train.reshape(X_train.shape[0], 64, 64, 1)
reshaped_x_test = X_test.reshape(X_test.shape[0], 64, 64, 1)

"""## Data Augmentation"""

train_datagen = ImageDataGenerator(
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        width_shift_range=0.3,
        height_shift_range=0.3,
        brightness_range=[0.2, 1.0])

test_datagen = ImageDataGenerator()

train_generator = train_datagen.flow(x=reshaped_x_train,
                               y=y_train,
                               batch_size=32,
                               shuffle=False,
                               seed=RANDOM_SEED)

test_generator = test_datagen.flow(
        x=reshaped_x_test,
        y=y_test)

"""# Function and Data Exploration"""

# from keras.models import Sequential
# from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D
# from keras.optimizers import SGD
# from keras.preprocessing.image import ImageDataGenerator
# from keras import backend as K
import tflearn
from tflearn.data_utils import shuffle, to_categorical
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.estimator import regression
from tflearn.data_preprocessing import ImagePreprocessing
from tflearn.data_augmentation import ImageAugmentation
from tflearn.metrics import Accuracy

from matplotlib import pyplot as plt  # for display of images
def show_color_image(image):
    plt.imshow(image)
    plt.axis('off')
    plt.show()
 
 
# Examine first cat and first dog color images
show_color_image(cats_1000_64_64_1[3,:,:,0])
show_color_image(dogs_1000_64_64_1[4,:,:,0])

#create train /test data for model 
X_cat_dog_m = np.concatenate((cats_1000_64_64_1, dogs_1000_64_64_1), axis = 0) 
#Define the labels to be used 1000 cats = 0 1000 dogs = 1
y_cat_dog_m = np.concatenate((np.zeros((1000), dtype = np.int32), 
                      np.ones((1000), dtype = np.int32)), axis = 0)
X_cat_dog_m.shape #check data shape

y_cat_dog_m.shape #check data shape

# # test-train split   
# X_train, X_test, y_train, y_test =  train_test_split(X_cat_dog_m, y_cat_dog_m, test_size=0.2, random_state=RANDOM_SEED)

# # encode the Ys
# y_train = to_categorical(y_train, 2)
# y_test = to_categorical(y_test, 2)

# print('X_train shape:', X_train.shape)
# print('X_test shape:', X_test.shape)
# print('y_train shape:', y_train.shape)
# print('y_test shape:', y_test.shape)
# print('Number of images in x_train', X_train.shape[0])
# print('Number of images in x_test', X_test.shape[0])

"""# Model Experimentation

## Common Functions
"""

from functools import partial

def fitted_model_printouts(model, optimizer, n_epochs):
    """
    This takes a constructed model as the main input.
    The model then compiles it based on the given optimizer, fits the model
    to the given epochs, and finally returns the test accuracy.
    """
    # compile
    model.compile(optimizer=optimizer,
                  loss=keras.losses.sparse_categorical_crossentropy,
                  metrics=['accuracy'])

    # fit
    model.fit(reshaped_x_train, y_train, epochs=n_epochs, verbose=2)

    # print test results
    test_loss, test_acc = model.evaluate(reshaped_x_test, y_test, verbose=1)

    return test_acc

def augmented_fit_generator(model, n_epochs):
    """
    This function takes inputs a built model and the number of epochs. It finally
    returns the test accuracy of the model.

    The difference between this and the above function is it uses the altered 
    image data built through ImageDataGen.
    """
    model.compile(optimizer='adam', 
                  loss=keras.losses.sparse_categorical_crossentropy,
                  metrics=['accuracy'])
    
    model.fit_generator(train_generator, epochs=n_epochs)

    results = model.evaluate_generator(test_generator)
    # print(results)
    # print(results[0])
    # print(results[1])
    test_acc = results[1]
    return test_acc

def elapsed_mins(start_time, end_time):
    return((end_time - start_time) / 60)

# #build out the model

# # Input is a 64x64 image 
# network = input_data(shape=[None, 64, 64, 1],
#                      data_preprocessing=img_prep,
#                      data_augmentation=img_aug)

# # 1:layer with 32 nodes
# layer_1 = conv_2d(network, 32, 3, activation='relu', name='layer_1')

# # 2:  pooling layer
# network = max_pool_2d(layer_1, 2)

# # 3: layer with 64 nodes
# layer_2 = conv_2d(network, 64, 3, activation='relu', name='layer_2')

# # 4: layer with 64 nodes
# layer_3 = conv_2d(conv_2, 64, 3, activation='relu', name='layer_3')

# # 5:  pooling layer
# network = max_pool_2d(layer_3, 2)

# # 6: Fully-connected 256 node layer
# network = fully_connected(network, 256, activation='relu')

# # 7: Dropout
# network = dropout(network, 0.5)

# # 8: Fully-connected layer with two outputs
# network = fully_connected(network, 2, activation='softmax')

# # Configure how the network will be trained
# acc = Accuracy(name="Accuracy")
# network = regression(network, optimizer='adam',
#                      loss='categorical_crossentropy',
#                      learning_rate=0.0005, metric=acc)

# # Wrap the network in a model object
# n_model = tflearn.DNN(network, max_checkpoints = 3, tensorboard_verbose = 3)

# Train model for 25 epochs

#n_model.fit(X_train, y_train, validation_set=(X_test, y_test), n_epoch=25, batch_size=500)

###unfortantly was not able to get this error resolved

"""## Base Models

### AlexNet Architecture
"""

# Build model architecture 

# AlexNet

alex = Sequential()
alex.add(Conv2D(64, 11, strides=2, padding='valid', # input layer
                activation='relu', input_shape=[64, 64, 1]))
alex.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex.add(Conv2D(256, kernel_size=(5,5), strides=1, padding='same', activation='relu'))
alex.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex.add(Conv2D(384, 3, strides=1, padding='same', activation='relu'))
alex.add(Conv2D(384, 3, strides=1, padding='same', activation='relu'))
alex.add(Conv2D(256, 3, strides=1, padding='same', activation='relu'))
alex.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex.add(Flatten())
alex.add(Dense(4096, activation='relu'))
alex.add(Dropout(0.5))
alex.add(Dense(4096, activation='relu'))
alex.add(Dropout(0.5))
alex.add(Dense(1000, activation='relu'))
alex.add(Dense(n_outputs, activation='softmax'))

adam_optimizer = keras.optimizers.Adam()
base_alex_start = time.time()
alex.compile(optimizer=adam_optimizer, 
             loss=keras.losses.sparse_categorical_crossentropy,
             metrics=['accuracy'])

alex.fit(reshaped_x_train, y_train, epochs=50, verbose=1)

base_alex_test_loss, base_alex_test_acc = alex.evaluate(reshaped_x_test, y_test, verbose=2)
base_alex_end = time.time()

base_alex_runtime = elapsed_mins(base_alex_start, base_alex_end)

"""### LeNet Architecture"""

lenet = Sequential()
lenet.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='relu', input_shape=[64,64,1]))

lenet.add(AveragePooling2D())
lenet.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))

lenet.add(AveragePooling2D())

lenet.add(Flatten())

lenet.add(Dense(units=120, activation='relu', 
                kernel_regularizer=None))
lenet.add(Dropout(0.5))

lenet.add(Dense(units=84, activation='relu'))
lenet.add(Dropout(0.5))

lenet.add(Dense(units=n_outputs, activation='softmax'))

base_lenet_start = time.time()
lenet.compile(optimizer='adam',
             loss=keras.losses.sparse_categorical_crossentropy,
             metrics=['accuracy'])
lenet.fit(reshaped_x_train, y_train, epochs=50, verbose=1)
base_lenet_test_loss, base_lenet_test_acc = lenet.evaluate(reshaped_x_test, y_test, verbose=2)
base_lenet_end = time.time()

base_lenet_runtime = elapsed_mins(base_lenet_start, base_lenet_end)

"""### Noah"""

# n_model = []
# n_model = Sequential()

# # Adds a densely-connected layer with 64 units to the model:
# n_model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = input_shape))
# n_model.add(MaxPooling2D(pool_size = (2,2)))

# # Add another:
# n_model.add(Conv2D(64, (3,3), activation = 'relu'))
# n_model.add(MaxPooling2D(pool_size = (2,2)))


# # Flatten layers and add dropout
# n_model.add(Flatten())
# n_model.add(Dense(64, activation='relu'))
# n_model.add(Dropout(0.5))

# # Add a softmax layer with 10 output units:
# n_model.add(Dense(1, activation='sigmoid')) #as using binary crossentropy loss should be set to 1 as either zero or 1 score can be returned in final layer


# #compile model
# opt = SGD(lr=.001,momentum = .9)
# n_model.compile(optimizer=opt,
#               loss='binary_crossentropy',
#               metrics=['accuracy'])

# n_model.summary()

# # Fit the model
# n_model.fit(X_train, y_train, epochs=20)

# test_loss, test_acc = n_model.evaluate(X_test, y_test, verbose=2)

# print('Test accuracy', test_acc)
# print('Test loss', test_loss)

#re run the model with data augmentation to compare scores
#history= model.fit_generator(train_generator, epochs=25)

"""Our thought process here was to run a model without data augmentation applied to same model with data augmentation and compare results to see if improves accuracy

## Vani

Worked with base AlexNet architecture.
"""

#Vani code 1
alex_model1 = Sequential()
alex_model1.add(Conv2D(64, kernel_size =3, padding='valid', activation='relu', input_shape=[64, 64, 1]))
alex_model1.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model1.add(Conv2D(256, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model1.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model1.add(Conv2D(384, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model1.add(Flatten())
alex_model1.add(Dense(n_outputs, activation='softmax'))

adam_op = keras.optimizers.Adam(lr=0.001)

model1_start = time.time()
alex_model1.compile(optimizer='adam', 
             loss=keras.losses.sparse_categorical_crossentropy,
             metrics=['accuracy'])

alex_model1.fit(reshaped_x_train, y_train, epochs=50, verbose=1)

model1_test_loss, model1_test_acc = alex_model1.evaluate(reshaped_x_test, y_test, verbose=2)
model1_end = time.time()

model1_runtime = elapsed_mins(model1_start, model1_end)
print("Model 1 Runtime: {}".format(model1_runtime))

# Rerun above code with data augmentation
n_epochs = 50
# Rerun above model with data augmentation
model1_aug_start = time.time()
model1_aug_acc = augmented_fit_generator(alex_model1, n_epochs)
model1_aug_end = time.time()
model1_aug_runtime = elapsed_mins(model1_aug_start, model1_aug_end)
print('Test Accuracy: {}'.format(model1_aug_acc))
print('Runtime: {}'.format(model1_aug_runtime))

# Added kernel initializer: He Normal.
alex_model2 = Sequential()
alex_model2.add(Conv2D(64, kernel_size =3, padding='valid', kernel_initializer='he_normal',
                bias_initializer='zeros', activation='relu', input_shape=[64, 64, 1]))
alex_model2.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model2.add(Conv2D(256, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model2.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model2.add(Conv2D(384, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model2.add(Flatten())
alex_model2.add(Dense(n_outputs, activation='softmax'))

adam_op = keras.optimizers.Adam(lr=1.0e-4)

model2_start = time.time()
alex_model2.compile(optimizer=adam_op, 
             loss=keras.losses.sparse_categorical_crossentropy,
             metrics=['accuracy'])

alex_model2.fit(reshaped_x_train, y_train, batch_size=128, epochs=50, verbose=1)

model2_test_loss, model2_test_acc = alex_model2.evaluate(reshaped_x_test, y_test, verbose=1)

model2_end = time.time()

model2_runtime = elapsed_mins(model2_start, model2_end)
print("Model 2 Runtime: {}".format(model2_runtime))

# Rerun above model with data augmentation
model2_aug_start = time.time()
model2_aug_acc = augmented_fit_generator(alex_model2, n_epochs)
model2_aug_end = time.time()
model2_aug_runtime = elapsed_mins(model2_aug_start, model2_aug_end)
print('Test Accuracy: {}'.format(model2_aug_acc))
print('Runtime: {}'.format(model2_aug_runtime))

# Added dropout
alex_model3 = Sequential()
alex_model3.add(Conv2D(64, kernel_size =3, padding='valid', kernel_initializer='he_normal',
                bias_initializer='zeros', activation='relu', input_shape=[64, 64, 1]))
alex_model3.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model3.add(Conv2D(256, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model3.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model3.add(Conv2D(384, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model3.add(Dropout(0.5)) # Added one more dropout layer here
alex_model3.add(Flatten())
alex_model3.add(Dense(n_outputs, activation='softmax'))

adam_op = keras.optimizers.Adam(lr=1.0e-4)

model3_start = time.time()

alex_model3.compile(optimizer=adam_op, 
             loss=keras.losses.sparse_categorical_crossentropy,
             metrics=['accuracy'])

alex_model3.fit(reshaped_x_train, y_train, batch_size=128, epochs=50, verbose=1)

model3_test_loss, model3_test_acc = alex_model3.evaluate(reshaped_x_test, y_test, verbose=2)
model3_end = time.time()

model3_runtime = elapsed_mins(model3_start, model3_end)
print("Model 3 Runtime: {}".format(model3_runtime))

# Added second dropout layer
alex_model4 = Sequential()
alex_model4.add(Conv2D(64, kernel_size =3, padding='valid', kernel_initializer='he_normal',
                bias_initializer='zeros', activation='relu', input_shape=[64, 64, 1]))
alex_model4.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model4.add(Conv2D(256, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model4.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_model4.add(Dropout(0.5)) # Added second droput layer here
alex_model4.add(Conv2D(384, kernel_size =3, strides=1, padding='same', activation='relu'))
alex_model4.add(Dropout(0.5)) # Added one more dropout layer here
alex_model4.add(Flatten())
alex_model4.add(Dense(n_outputs, activation='softmax'))

adam_op = keras.optimizers.Adam(lr=1.0e-4)

model4_start = time.time()
alex_model4.compile(optimizer=adam_op, 
             loss=keras.losses.sparse_categorical_crossentropy,
             metrics=['accuracy'])

alex_model4.fit(reshaped_x_train, y_train, batch_size=128, epochs=50, verbose=1)

model4_test_loss, model4_test_acc = alex_model4.evaluate(reshaped_x_test, y_test, verbose=2)
model4_end = time.time()

model4_runtime = elapsed_mins(model4_start, model4_end)
print("Model 4 Runtime: {}".format(model4_runtime))

"""Conclusions:
* Data augmentation is not helping in these models. 
* The best model uses He initialization.
* Modifying the architecture with dropout layers did not improve accuracies.

## Ali
"""

# # Scikit Learn for random splitting of the data  
# from sklearn.model_selection import train_test_split

# # Random splitting of the data in to training (80%) and test (20%)  
# X_train, X_test, y_train_1, y_test_1 = \
#     train_test_split(X_cat_dog_min_max, y_cat_dog, test_size=0.20, 
#                      random_state = RANDOM_SEED)

# # Shape the X_train and X_test data.
# X_train_1 = X_train.reshape(X_train.shape[0], 64, 64, 1)
# X_test_1 = X_test.reshape(X_test.shape[0], 64, 64, 1)
input_shape = (64, 64, 1)

# # Making sure that the values are float so that we can get decimal points after division
# X_train_1 = X_train_1.astype('float32')
# X_test_1 = X_test_1.astype('float32')

# print('X_train shape:', X_train_1.shape)
# print('X_test shape:', X_test_1.shape)
# print('y_train shape:', y_train_1.shape)
# print('y_test shape:', y_test_1.shape)
# print('Number of images in x_train', X_train_1.shape[0])
# print('Number of images in x_test', X_test_1.shape[0])

# # Build the model
# from keras.models import Sequential
# from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D
# # from keras.optimizers import SGD

model_relu_adam_binary_sigmoid = []
model_relu_adam_binary_sigmoid = Sequential()

# Adds a densely-connected layer with 64 units to the model:
model_relu_adam_binary_sigmoid.add(Conv2D(32, (3,3), activation = 'relu', input_shape = input_shape))
model_relu_adam_binary_sigmoid.add(MaxPooling2D(pool_size = (2,2)))

# Add another:
model_relu_adam_binary_sigmoid.add(Conv2D(32, (3,3), activation = 'relu'))
model_relu_adam_binary_sigmoid.add(MaxPooling2D(pool_size = (2,2)))

# Add another:
model_relu_adam_binary_sigmoid.add(Conv2D(64, (3,3), activation = 'relu'))
model_relu_adam_binary_sigmoid.add(MaxPooling2D(pool_size = (2,2)))

# Flatten layers and add dropout
model_relu_adam_binary_sigmoid.add(Flatten())
model_relu_adam_binary_sigmoid.add(Dense(64, activation='relu'))
model_relu_adam_binary_sigmoid.add(Dropout(0.5))

# Add a softmax layer with 10 output units:
model_relu_adam_binary_sigmoid.add(Dense(1, activation='sigmoid'))

model_relu_adam_binary_sigmoid.compile(optimizer="adam",
              loss='binary_crossentropy',
              metrics=['accuracy'])

model_relu_adam_binary_sigmoid.summary()

# Fit the model
model_relu_adam_binary_sigmoid.fit(reshaped_x_train, y_train, epochs=30)

test_loss_model_relu_adam_binary_sigmoid, test_acc_model_relu_adam_binary_sigmoid = model_relu_adam_binary_sigmoid.evaluate(reshaped_x_test, y_test, verbose=2)

print('Test accuracy', test_acc_model_relu_adam_binary_sigmoid)
print('Test loss', test_loss_model_relu_adam_binary_sigmoid)

"""## Ben"""

adam_op = keras.optimizers.Adam()

n_epochs = 50

# Reuse base AlexNet model, replace all activations with selu

alex_selu = Sequential()
alex_selu.add(Conv2D(64, 11, strides=2, padding='valid', # input layer
                activation='selu', input_shape=[64, 64, 1]))
alex_selu.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_selu.add(Conv2D(256, (5,5), strides=1, padding='same', activation='selu'))
alex_selu.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_selu.add(Conv2D(384, 3, strides=1, padding='same', activation='selu'))
alex_selu.add(Conv2D(384, 3, strides=1, padding='same', activation='selu'))
alex_selu.add(Conv2D(256, 3, strides=1, padding='same', activation='selu'))
alex_selu.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))
alex_selu.add(Flatten())
alex_selu.add(Dense(4096, activation='selu'))
alex_selu.add(Dropout(0.5))
alex_selu.add(Dense(4096, activation='selu'))
alex_selu.add(Dropout(0.5))
alex_selu.add(Dense(1000, activation='selu'))
alex_selu.add(Dense(n_outputs, activation='softmax'))

alex_selu_start = time.time()
alex_selu_acc = fitted_model_printouts(alex_selu, adam_op, n_epochs)
alex_selu_end = time.time()

alex_selu_runtime = elapsed_mins(alex_selu_start, alex_selu_end)

print("Model accuracy: {}".format(alex_selu_acc))
print("Model runtime: {}".format(alex_selu_runtime))

# Rerun above model with data augmentation
alex_selu_aug_start = time.time()
alex_selu_aug_acc = augmented_fit_generator(alex_selu, n_epochs)
alex_selu_aug_end = time.time()
alex_selu_aug_runtime = elapsed_mins(alex_selu_aug_start, alex_selu_aug_end)
print('Test Accuracy: {}'.format(alex_selu_aug_acc))
print('Runtime: {}'.format(alex_selu_aug_runtime))

"""Data augmentation made no difference in the results except lengthen the runtime."""

# Rerun Alex with selu activation with different learning rate
adam_op2 = keras.optimizers.Adam(lr=0.003)
alex_selu2_start = time.time()
alex_selu2_acc = fitted_model_printouts(alex_selu, adam_op2, n_epochs)
alex_selu2_end = time.time()

alex_selu2_runtime = elapsed_mins(alex_selu2_start, alex_selu2_end)

print("Model accuracy: {}".format(alex_selu2_acc))
print("Model runtime: {}".format(alex_selu2_runtime))

# First, try a different learning rate with base LeNet architecture
# adam_op2 is referenced in the above cell
lenet2_start = time.time()
lenet2_acc = fitted_model_printouts(lenet, adam_op2, n_epochs)
lenet2_end = time.time()
lenet2_runtime = elapsed_mins(lenet2_start, lenet2_end)

print("Model accuracy: {}".format(lenet2_acc))
print("Model runtime: {}".format(lenet2_runtime))

"""The new learning rate has much worse accuracy."""

# Base architecture, selu activation

lenet_selu = Sequential()
lenet_selu.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu.add(AveragePooling2D())
lenet_selu.add(Conv2D(filters=16, kernel_size=(3, 3), activation='selu'))

lenet_selu.add(AveragePooling2D())

lenet_selu.add(Flatten())

lenet_selu.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu.add(Dropout(0.5))

lenet_selu.add(Dense(units=84, activation='selu'))
lenet_selu.add(Dropout(0.5))

lenet_selu.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu_start = time.time()
lenet_selu_acc = fitted_model_printouts(lenet_selu, adam_op, n_epochs)
lenet_selu_end = time.time()

lenet_selu_runtime = elapsed_mins(lenet_selu_start, lenet_selu_end)

print("Model accuracy: {}".format(lenet_selu_acc))
print("Model runtime: {}".format(lenet_selu_runtime))

"""selu activation slightly increased model accuracy."""

# Rerun above model with data augmentation
lenet_selu_aug_start = time.time()
lenet_selu_aug_acc = augmented_fit_generator(lenet_selu, n_epochs)
lenet_selu_aug_end = time.time()
lenet_selu_aug_runtime = elapsed_mins(lenet_selu_aug_start, lenet_selu_aug_end)
print('Test Accuracy: {}'.format(lenet_selu_aug_acc))
print('Runtime: {}'.format(lenet_selu_aug_runtime))

"""Again, this version of data augmentation is not producing better results."""

# Try a different learning rate from previous lenet selu model
lenet_selu2_start = time.time()
lenet_selu2_acc = fitted_model_printouts(lenet_selu, adam_op2, 50)
lenet_selu2_end = time.time()
lenet_selu2_runtime = elapsed_mins(lenet_selu2_start, lenet_selu2_end)
print('Test Accuracy: {}'.format(lenet_selu2_acc))
print('Runtime: {}'.format(lenet_selu2_runtime))

"""Again, model accuracy is not improved with the changed learning rate. Moving along, I will use the default Adam optimizer with default learning rate."""

# LeNet architecture
# Skipped lenet_selu3 because it was the same as lenet_selu

# selu4 adds a convolution layer in line 13

lenet_selu4 = Sequential()
lenet_selu4.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu4.add(AveragePooling2D())
lenet_selu4.add(Conv2D(filters=32, kernel_size=(3, 3), activation='selu'))

lenet_selu4.add(Conv2D(filters=32, kernel_size=(3, 3), activation='selu')) # 2 conv2d before avg pooling layer

lenet_selu4.add(AveragePooling2D())

lenet_selu4.add(Flatten())

lenet_selu4.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu4.add(Dropout(0.5))

lenet_selu4.add(Dense(units=84, activation='selu'))
lenet_selu4.add(Dropout(0.5))

lenet_selu4.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu4_start = time.time()
lenet_selu4_acc = fitted_model_printouts(lenet_selu4, adam_op, n_epochs)
lenet_selu4_end = time.time()

lenet_selu4_runtime = elapsed_mins(lenet_selu4_start, lenet_selu4_end)

print("Model accuracy: {}".format(lenet_selu4_acc))
print("Model runtime: {}".format(lenet_selu4_runtime))

"""Adding the convolution layer slightly decreased accuracy from the base selu activation model."""

# LeNet architecture
# changed kernel size from selu4 to 2,2 instead of 3,3
lenet_selu5 = Sequential()
lenet_selu5.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu5.add(AveragePooling2D())
lenet_selu5.add(Conv2D(filters=32, kernel_size=(2, 2), activation='selu'))

lenet_selu5.add(Conv2D(filters=32, kernel_size=(2, 2), activation='selu')) # 2 conv2d before avg pooling layer

lenet_selu5.add(AveragePooling2D())

lenet_selu5.add(Flatten())

lenet_selu5.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu5.add(Dropout(0.5))

lenet_selu5.add(Dense(units=84, activation='selu'))
lenet_selu5.add(Dropout(0.5))

lenet_selu5.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu5_start = time.time()
lenet_selu5_acc = fitted_model_printouts(lenet_selu5, adam_op, n_epochs)
lenet_selu5_end = time.time()

lenet_selu5_runtime = elapsed_mins(lenet_selu5_start, lenet_selu5_end)

print("Model accuracy: {}".format(lenet_selu5_acc))
print("Model runtime: {}".format(lenet_selu5_runtime))

"""A slightly smaller kernel size made for better accuracy."""

# LeNet architecture
# took selu4 and added another convlution layer in line 13, after the first additional one

lenet_selu7 = Sequential()
lenet_selu7.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu7.add(AveragePooling2D())
lenet_selu7.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu'))

lenet_selu7.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) # 2 conv2d before avg pooling layer

lenet_selu7.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) # 3rd conv2d layer

lenet_selu7.add(AveragePooling2D())

lenet_selu7.add(Flatten())

lenet_selu7.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu7.add(Dropout(0.5))

lenet_selu7.add(Dense(units=84, activation='selu'))
lenet_selu7.add(Dropout(0.5))

lenet_selu7.add(Dense(units=n_outputs, activation='softmax'))

#print(fitted_model_printouts(lenet_selu7, adam_op, n_epochs))

lenet_selu7_start = time.time()
lenet_selu7_acc = fitted_model_printouts(lenet_selu7, adam_op, n_epochs)
lenet_selu7_end = time.time()

lenet_selu7_runtime = elapsed_mins(lenet_selu7_start, lenet_selu7_end)

print("Model accuracy: {}".format(lenet_selu7_acc))
print("Model runtime: {}".format(lenet_selu7_runtime))

"""Conv, Conv, Conv, Pooling did decreases accuracy."""

# LeNet architecture

# added a pooling layer between additional 2nd and 3rd conv layer form selu7

lenet_selu8 = Sequential()
lenet_selu8.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu8.add(AveragePooling2D())
lenet_selu8.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu'))

lenet_selu8.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) 

lenet_selu8.add(AveragePooling2D()) # add pooling layer after 2nd conv2d

lenet_selu8.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) 

lenet_selu8.add(AveragePooling2D())

lenet_selu8.add(Flatten())

lenet_selu8.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu8.add(Dropout(0.5))

lenet_selu8.add(Dense(units=84, activation='selu'))
lenet_selu8.add(Dropout(0.5))

lenet_selu8.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu8_start = time.time()
lenet_selu8_acc = fitted_model_printouts(lenet_selu8, adam_op, n_epochs)
lenet_selu8_end = time.time()

lenet_selu8_runtime = elapsed_mins(lenet_selu8_start, lenet_selu8_end)

print("Model accuracy: {}".format(lenet_selu8_acc))
print("Model runtime: {}".format(lenet_selu8_runtime))

"""Adding a pooling layer between layers helped accuracy here."""

# LeNet architecture
# Changing kernel size from 3,3 to 1,1 from selu8
lenet_selu9 = Sequential()
lenet_selu9.add(Conv2D(filters=6, kernel_size=(1,1), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu9.add(AveragePooling2D())
lenet_selu9.add(Conv2D(filters=64, kernel_size=(1, 1), activation='selu'))

lenet_selu9.add(Conv2D(filters=64, kernel_size=(1, 1), activation='selu')) 

lenet_selu9.add(AveragePooling2D()) # add pooling layer after 2nd conv2d

lenet_selu9.add(Conv2D(filters=64, kernel_size=(1, 1), activation='selu')) 

lenet_selu9.add(AveragePooling2D())

lenet_selu9.add(Flatten())

lenet_selu9.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu9.add(Dropout(0.5))

lenet_selu9.add(Dense(units=84, activation='selu'))
lenet_selu9.add(Dropout(0.5))

lenet_selu9.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu9_start = time.time()
lenet_selu9_acc = fitted_model_printouts(lenet_selu9, adam_op, n_epochs)
lenet_selu9_end = time.time()

lenet_selu9_runtime = elapsed_mins(lenet_selu9_start, lenet_selu9_end)

print("Model accuracy: {}".format(lenet_selu9_acc))
print("Model runtime: {}".format(lenet_selu9_runtime))

"""But changing kernel size to 1 did not do much."""

# LeNet architecture
# Changing kernel size from 3,3 to 5,5
# Same architecture as selu8
lenet_selu10 = Sequential()
lenet_selu10.add(Conv2D(filters=6, kernel_size=(5,5), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu10.add(AveragePooling2D())
lenet_selu10.add(Conv2D(filters=64, kernel_size=(5,5), activation='selu'))

lenet_selu10.add(Conv2D(filters=64, kernel_size=(5,5), activation='selu')) 

lenet_selu10.add(AveragePooling2D()) # add pooling layer after 2nd conv2d

lenet_selu10.add(Conv2D(filters=64, kernel_size=(5,5), activation='selu')) 

lenet_selu10.add(AveragePooling2D())

lenet_selu10.add(Flatten())

lenet_selu10.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu10.add(Dropout(0.5))

lenet_selu10.add(Dense(units=84, activation='selu'))
lenet_selu10.add(Dropout(0.5))

lenet_selu10.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu10_start = time.time()
lenet_selu10_acc = fitted_model_printouts(lenet_selu10, adam_op, n_epochs)
lenet_selu10_end = time.time()

lenet_selu10_runtime = elapsed_mins(lenet_selu10_start, lenet_selu10_end)

print("Model accuracy: {}".format(lenet_selu10_acc))
print("Model runtime: {}".format(lenet_selu10_runtime))

"""Changing to a large kernel size decreased accuracy significantly."""

# LeNet architecture
# Move pooling layer from after input to after first conv2d
# Changed from selu8
lenet_selu11 = Sequential()
lenet_selu11.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu11.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu'))

lenet_selu11.add(AveragePooling2D()) # moved avg poolng layer

lenet_selu11.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) # 2 conv2d before avg pooling layer

lenet_selu11.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) # 3rd conv2d layer

lenet_selu11.add(AveragePooling2D())

lenet_selu11.add(Flatten())

lenet_selu11.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu11.add(Dropout(0.5))

lenet_selu11.add(Dense(units=84, activation='selu'))
lenet_selu11.add(Dropout(0.5))

lenet_selu11.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu11_start = time.time()
lenet_selu11_acc = fitted_model_printouts(lenet_selu11, adam_op, n_epochs)
lenet_selu11_end = time.time()

lenet_selu11_runtime = elapsed_mins(lenet_selu11_start, lenet_selu11_end)

print("Model accuracy: {}".format(lenet_selu11_acc))
print("Model runtime: {}".format(lenet_selu11_runtime))

"""Moving the pooling layer did not change much either."""

# LeNet architecture
# added one more convolution layer in line 15
lenet_selu13 = Sequential()
lenet_selu13.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu13.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu'))

lenet_selu13.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) # 2 conv2d before avg pooling layer

lenet_selu13.add(AveragePooling2D()) # moved avg poolng layer

lenet_selu13.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) # 3rd conv2d layer

lenet_selu13.add(Conv2D(filters=64, kernel_size=(3, 3), activation='selu')) # 4th conv2d layer

lenet_selu13.add(AveragePooling2D())

lenet_selu13.add(Flatten())

lenet_selu13.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu13.add(Dropout(0.5))

lenet_selu13.add(Dense(units=84, activation='selu'))
lenet_selu13.add(Dropout(0.5))

lenet_selu13.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu13_start = time.time()
lenet_selu13_acc = fitted_model_printouts(lenet_selu13, adam_op, n_epochs)
lenet_selu13_end = time.time()

lenet_selu13_runtime = elapsed_mins(lenet_selu13_start, lenet_selu13_end)

print("Model accuracy: {}".format(lenet_selu13_acc))
print("Model runtime: {}".format(lenet_selu13_runtime))

"""Adding another convolution layer is again decreasing accuracy."""

# LeNet architecture
# same as model selu13, changing filter size to 128
lenet_selu14 = Sequential()
lenet_selu14.add(Conv2D(filters=6, kernel_size=(3,3), 
                 activation='selu', input_shape=[64,64,1]))

lenet_selu14.add(Conv2D(filters=128, kernel_size=(3, 3), activation='selu'))

lenet_selu14.add(Conv2D(filters=128, kernel_size=(3, 3), activation='selu')) # 2 conv2d before avg pooling layer

lenet_selu14.add(AveragePooling2D()) # moved avg poolng layer

lenet_selu14.add(Conv2D(filters=128, kernel_size=(3, 3), activation='selu')) # 3rd conv2d layer

lenet_selu14.add(Conv2D(filters=128, kernel_size=(3, 3), activation='selu')) # 4th conv2d layer

lenet_selu14.add(AveragePooling2D())

lenet_selu14.add(Flatten())

lenet_selu14.add(Dense(units=120, activation='selu', 
                kernel_regularizer=None))
lenet_selu14.add(Dropout(0.5))

lenet_selu14.add(Dense(units=84, activation='selu'))
lenet_selu14.add(Dropout(0.5))

lenet_selu14.add(Dense(units=n_outputs, activation='softmax'))

lenet_selu14_start = time.time()
lenet_selu14_acc = fitted_model_printouts(lenet_selu14, adam_op, n_epochs)
lenet_selu14_end = time.time()

lenet_selu14_runtime = elapsed_mins(lenet_selu14_start, lenet_selu14_end)

print("Model accuracy: {}".format(lenet_selu14_acc))
print("Model runtime: {}".format(lenet_selu14_runtime))

"""# Summary"""

summary_dict = [
    {
        'model': 'base_alex',
        'accuracy': base_alex_test_acc,
        'runtime': base_alex_runtime
    },
    {
        'model': 'base_lenet',
        'accuracy': base_lenet_test_acc,
        'runtime': base_lenet_runtime
    },
    {
        'model': 'model1',
        'accuracy': model1_test_acc,
        'runtime': model1_runtime
    },
    {
        'model': 'model1_with_augmentation',
        'accuracy': model1_aug_acc,
        'runtime': model1_aug_acc
    },
        {
        'model': 'model2',
        'accuracy': model2_test_acc,
        'runtime': model2_runtime
    },
    {
        'model': 'model2_with_augmentation',
        'accuracy': model2_aug_acc,
        'runtime': model2_aug_acc
    },
    {
        'model': 'model3',
        'accuracy': model3_test_acc,
        'runtime': model3_runtime
    },
    {
        'model': 'model4',
        'accuracy': model4_test_acc,
        'runtime': model4_runtime
    },
    {
        'model': 'model5',
        'accuracy': alex_selu_acc,
        'runtime': alex_selu_runtime
    },
        {
        'model': 'model5_with_augmentation',
        'accuracy': alex_selu_aug_acc,
        'runtime': alex_selu_aug_runtime
    },
    {
        'model': 'model6',
        'accuracy': alex_selu2_acc,
        'runtime': alex_selu2_runtime
    },
    {
        'model': 'model7',
        'accuracy': lenet2_acc,
        'runtime': lenet2_runtime
    },
    {
        'model': 'model8',
        'accuracy': lenet_selu_acc,
        'runtime': lenet_selu_runtime
    },
    {
        'model': 'model8_with_augmentation',
        'accuracy': lenet_selu_aug_acc,
        'runtime': lenet_selu_aug_runtime
    },
    {
        'model': 'model9',
        'accuracy': lenet_selu2_acc,
        'runtime': lenet_selu2_runtime
    },
    {
        'model': 'model10',
        'accuracy': lenet_selu4_acc,
        'runtime': lenet_selu4_runtime
    },
    {
        'model': 'model11',
        'accuracy': lenet_selu5_acc,
        'runtime': lenet_selu5_runtime
    },
    {
        'model': 'model12',
        'accuracy': lenet_selu7_acc,
        'runtime': lenet_selu7_runtime
    },
    {
        'model': 'model13',
        'accuracy': lenet_selu8_acc,
        'runtime': lenet_selu8_runtime
    },
    {
        'model': 'model14',
        'accuracy': lenet_selu9_acc,
        'runtime': lenet_selu9_runtime
    },
    {
        'model': 'model15',
        'accuracy': lenet_selu10_acc,
        'runtime': lenet_selu10_runtime
    },
    {
        'model': 'model16',
        'accuracy': lenet_selu11_acc,
        'runtime': lenet_selu11_runtime
    },
    {
        'model': 'model18',
        'accuracy': lenet_selu13_acc,
        'runtime': lenet_selu13_runtime
    },
    {
        'model': 'model19',
        'accuracy': lenet_selu14_acc,
        'runtime': lenet_selu14_runtime
    }
]

df = pd.DataFrame.from_dict(summary_dict)
df

"""## Conclusion

* More layers does not always equate to better accuracy. 
* Models 1-4 use AlexNet architecture.
* Models 5-19 use the LeNet architecture.

# Script End
"""

script_end = time.time()

script_runtime = elapsed_mins(script_start, script_end)
print('Total script runtime in minutes: {}'.format(script_runtime))

"""# Appendix

* https://www.learnpython.org/en/Partial_functions

# Notes

* Save every model separately.
* Other things to play around with:
    * regularization: https://www.tensorflow.org/api_docs/python/tf/keras/regularizers
    * activation: https://www.tensorflow.org/api_docs/python/tf/keras/activations

* From text: SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic

# References

* http://www.subsubroutine.com/sub-subroutine/2016/9/30/cats-and-dogs-and-convolutional-neural-networks

* https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c02_dogs_vs_cats_with_augmentation.ipynb#scrollTo=8CfngybnFHQR

* https://towardsdatascience.com/cat-or-dog-image-classification-with-convolutional-neural-network-d421a9363c7a

* AlexNet tutorial: https://engmrk.com/alexnet-implementation-using-keras/

* General CNN with Keras tutorial: https://keras.io/examples/mnist_cnn/

* LeNet tutorial: https://medium.com/@mgazar/lenet-5-in-9-lines-of-code-using-keras-ac99294c8086
"""